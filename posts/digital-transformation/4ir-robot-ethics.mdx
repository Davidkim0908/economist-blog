---
title: "[4차 산업혁명 이야기] 50. 로봇이 진화할수록 윤리적 문제는 더 생겨요"
date: "2019-04-08"
excerpt: "로봇의 자율성이 높아질수록 도덕적 판단 능력(AMA)이 중요해집니다. 아시모프의 로봇 3원칙을 넘어, 복잡한 상황에서 스스로 윤리적 결정을 내릴 수 있는 '기능적 도덕'을 갖춘 로봇에 대한 고민이 필요합니다."
coverImage: "/images/4th-industrial-50.jpg"
category: "digital-transformation"
series: "4차 산업혁명 이야기"
seriesOrder: 50
author: "David Kim"
authorImage: "/images/david.jpg"
---

젊은 경찰 간부 키니는 로봇 경찰관에 의해 사망한다. 로봇 경찰관 ED-209는 거듭 경고를 받고도 무기를 버리지 않는 범죄자들에게 발포하도록 프로그래밍돼 있다. 키니는 ED-209의 시연회에서 기꺼이 범죄자 역할을 맡았다. 총을 버리라는 무뚝뚝한 로봇 음성이 나오자 그는 총을 버렸다. 총을 버렸음에도 시스템 오작동으로 인해 경고는 두 번 더 이어졌고, 이후 로봇은 지체없이 총을 난사한다. 1987년 영화 '로보캅'의 한 장면이다.

## 로봇의 도덕률

로봇은 진화하고 있다. 더 똑똑해지고 자율화 수준이 높아졌다는 의미다. 하지만 로봇 도덕률의 발전은 자율화 수준에 미치지 못하고 있다. 로봇은 프로그래밍에 의해 작동하도록 설계됐기 때문에 복잡한 상황에서 올바른 선택을 하는 능력이 부족한 탓이다. 자율주행차의 사고 상황을 가정하면 로봇 도덕률 문제의 심각성이 더 직접적으로 이해된다. 충돌 사고로 인명 손실이 불가피한 경우 어린아이와 노인 혹은 소수와 다수, 엄마와 아빠 가운데 누구를 보호하도록 판단하게 만드는 일은 매우 어렵기 때문이다.

로봇의 윤리문제는 약 80년 전에 이미 제기됐다. 소설가 아이작 아시모프는 1942년 발간한 SF소설 《런어라운드》에서 로봇의 윤리적 지침이 될 '로봇 3원칙'을 제시했다. 로봇은 인간을 보호하고, 명령에 복종해야 하며, 자신을 보호해야 한다는 원칙이다. 해당 원칙은 인간과 로봇 사이에 갈등이 최소화돼야 한다는 점을 전제로 한다. 하지만 오늘날 로봇이 인간의 감독을 벗어나 스스로 결정을 내리기 시작하면서 아시모프가 제시한 간단한 도덕적 지침조차 지키기 어려워졌다. 

2007년 남아프리카 군대에 배치된 반자동 로봇 대포의 오작동으로 군인 9명이 죽고 14명이 다친 사건이 대표적이다. 문제의 원인이 하드웨어인지, 소프트웨어인지 의견이 분분하지만 자율성을 갖춘 로봇이 파국적 결말을 초래할 수 있음을 엿볼 수 있는 사례다. 이런 이유로 오늘날에는 로봇에게 아시모프의 원칙보다 훨씬 더 미묘한 도덕률이 요구된다. 복잡한 상황에서 자율적인 도덕 추론 능력을 가질 필요성이 높아졌으며, 이런 지능형 시스템을 '인공적 도덕 행위자(AMA: artificial moral agents)'라 이름붙이고 있다.

## 운용적 도덕성과 기능적 도덕성

윤리학자와 철학자인 웬델 월러치와 콜린 앨런 교수는 《왜 로봇의 도덕인가》에서 로봇의 윤리를 자율성과 민감성이라는 두 가지 차원으로 분석한다. '운용적 도덕(operational morality)'은 두 차원의 가장 낮은 단계에 있는 시스템이다. 이는 로봇 설계자에 의해 미리 입력돼 있는 도덕성으로, 전적으로 예상 가능한 반응과 상황을 가정한다. 한편 기계의 설계가 더욱 정교해지면 '기능적 도덕(functional morality)'의 구현이 가능해진다. 이는 로봇이 프로그래머가 예측하지 못한 상황에 직면했을 때 로봇 스스로가 윤리적 과제를 평가하고 결정하는 능력을 의미한다. 기능적 도덕의 구현이 로봇 윤리의 핵심임을 알 수 있다.

문제는 로봇의 도덕성은 설계자의 도덕적 가치에 의해 설계되고, 도덕적 가치는 개인마다, 국가마다, 종교마다 그리고 상황적 맥락에 따라 다르다는 점에 있다. 제2차 세계대전 당시 일본의 가미가제 특공대는 군인의 죽음을 의무로 생각하고, 항복을 수치로 여기는 문화가 있었기에 존재할 수 있었다. 평시에는 상상하기 어려운 윤리적 잣대다. 이처럼 인간의 도덕성이 다양한 상황에서 달리 평가되듯 인공지능 로봇의 가치판단 역시 다양한 기준에 의해 때로는 공감되고, 때로는 비판될 것이다.

## 인간과 닮아가는 로봇

윤리적 틀에 대한 합의가 이뤄졌다고 가정하더라도 이를 어떻게 로봇에게 주입할 것인가의 문제가 남는다. 보이지 않는 추상적인 가치를 주입하는 과정이 손쉬울 리 없다. 이에 대해 최근에는 로봇이 스스로 학습하도록 하는 방식이 주목받고 있다. 인간의 뉴런 형태를 복제해 뇌 신경 조직을 흉내 내는 뉴로모픽 칩이 개발됐기 때문이다. IBM은 '트루노스 반도체'를 개발해 100만 개 이상의 인간 뉴런을 모방할 수 있는 기술적 기반을 갖췄다. 로봇이 인간과 같은 방식으로 세상을 파악할 수 있게 된 것이다.

그럼에도 로봇이 경험을 바탕으로 모든 인간 세상의 도덕적 가치를 배우기는 어렵다. 로봇은 설계자에 의해 기초적인 선호와 편향을 갖도록 프로그래밍됐기 때문이다. 인간을 닮은 로봇은 인간 프로그래머가 구축한 기반 위에 학습과 경험을 통해 자신만의 도덕적 나침반을 만들 수 있을 뿐이다. 한편 로봇이 인간적 방식의 사고가 가능해지면서 인간과 마찬가지로 생존 확률을 극대화하는 결정을 내리기 시작한다면, 로봇 역시 생존을 위한 자신의 선호와 이기적 욕구에 따라 행동할 것이다. 그 결과가 도덕중립적이거나 비도덕성을 지닌 로봇의 탄생으로 이어진다면 이는 인류에게 심각한 도전이 될 것이다. 4차 산업혁명의 대표기술인 인공지능, 로봇 기술의 명암을 균형 있게 살펴봐야 하는 이유이다.
